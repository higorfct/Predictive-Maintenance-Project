# -*- coding: utf-8 -*-
"""11_2_Predictive__Maintenance .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oxbeQDX_dk09scA1HI3gw4olVexDHCT

### **Imports**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

"""### **Loading data**"""

df = pd.read_csv("failure_dataset.csv")
df

"""#**Defining X and Y**"""

X = df[["operation_time", "temperature", "vibration"]]
y = df["failures"]

"""#**Train_test_split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#**Pre-Processing**"""

scaler = StandardScaler()

# Fit the scaler only on the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data using the scaler fitted on the training data
X_test_scaled = scaler.transform(X_test)

# Display the shapes of the scaled dataframes to confirm
print("Shape of X_train_scaled:", X_train_scaled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)

"""###**Training the model**"""

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

"""#**Hyperparameter Tuning**"""

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
# Using the model defined previously (assuming it's still the RandomForestRegressor)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')

# Fit the grid search to the scaled training data
# Assuming X_train_scaled and y_train are available from previous steps
# If not, you'll need to split and scale the data first.
grid_search.fit(X_train, y_train) # Fit on training data

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = -grid_search.best_score_ # Convert negative MAE to positive

print(f"Best parameters found: {best_params}")
print(f"Best MAE in the training set (with cross validation): {best_score:.2f}")

# Update the model with the best parameters
modelo_tuned = grid_search.best_estimator_

"""###**Predictions**"""

#Input time / temperature, vibration
user_input = pd.DataFrame([[9, 45, 2.5]], columns=["operation_time", "temperature", "vibration"])
# normalization
user_input_scaled = scaler.transform(user_input)
# Prediction
lambda_predicted = model.predict(user_input_scaled)[0]
print(f"Failure Prediction (λ): {lambda_predicted:.2f}")

"""###**Occurence Probabilites**"""

x_vals = np.arange(0, int(lambda_predicted + 3))
print(x_vals)

probs = poisson.pmf(x_vals, lambda_predicted)
print(probs)

z_vals = np.round(probs, 4)
print(z_vals)

labels = [f"{i} prob.: {round(p, 4)}" for i, p in zip(x_vals, probs)]
print(labels)

"""###**Graph of Probabilities**"""

theme_colors = [plt.cm.Greys(0.2 + 0.6 * (p / max(probs))) for p in probs]
fig, ax = plt.subplots()
ax.bar(x_vals, probs, tick_label=labels, color=theme_colors)
ax.set_title(f"Poisson Distribution for λ predicted: {lambda_predicted:.2f}")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()